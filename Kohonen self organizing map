def distance(W,X):
    D=0
    for i in range(0,len(W)):
        D=D+pow((W[i]-X[i]),2)
    D=round(D,2)
    return D

def update_weight(W,R,X):
    U_W=[]
    for i in range(0,len(W)):
        U_W.append(W[i]+(R*(X[i]-W[i])))
    return U_W

def print_weight(W,w_n):
    for i in range(0,w_n):
        print(f'W[{i}]={W[i]}')

def print_input(X,v):
    for i in range(0,v):
        print(f'X[{i}]={X[i]}')

W=[]
w_n=int(input('Input the number of weight vectors : '))
for i in range(0,w_n):
    w_i=list(map(float,input(f'Enter the weights of {i} weight vector :').strip().split()))
    W.append(w_i)

X=[]
v=int(input('Input the number of input vectors : '))
for i in range(0,v):
    x_i=list(map(float,input(f'Enter the inputs of {i} input vector :').strip().split()))
    X.append(x_i)


L_R=float(input('Enter the learning rate : '))
print('Initial Weights : ')
print_weight(W,w_n)
print('Initial Input Vectors : ')
print_input(X,v)

k=w_n
if (k==len(X[0])):
    p=[]
    for i in range(0,len(W[0])):
        l=[]
        for j in range(0,k):
            l.append(W[j][i])
        p.append(l)
    W=[]
    for i in range(0,len(p)):
        W.append(p[i])
    w_n=len(W)


D=[None]*w_n
for j in range(0,v):
    for i in range(0,w_n):
        D[i]=distance(W[i],X[j])
    min_d=D.index(min(D))
    W[min_d]=update_weight(W[min_d],L_R,X[j])

if (k==len(X[0])):
    q=[]
    for i in range(0,len(W[0])):
        l=[]
        for j in range(0,len(W)):
            l.append(W[j][i])
        q.append(l)
    W=[]
    for i in range(0,len(q)):
        W.append(q[i])
    w_n=len(W)
print('Final Weights : ')
print_weight(W,w_n)

# Input the number of weight vectors: 2
# Enter the weights of 0 weight vector: 1.0 0.9 0.7 0.5 0.3
# Enter the weights of 1 weight vector: 0.3 0.5 0.7 0.9 1.0
# Input the number of input vectors: 1
# Enter the inputs of 0 input vectors: 0.0 0.5 1.0 0.5 0.0
# Enter the learning rate: 2.5


# Self Organizing Map (or Kohonen Map or SOM) is a type of Artificial Neural Network
# which is also inspired by biological models of neural systems from the 1970s. It follows an
# unsupervised learning approach and trained its network through a competitive learning
# algorithm. SOM is used for clustering and mapping (or dimensionality reduction)
# techniques to map multidimensional data onto lower-dimensional which allows people to
# reduce complex problems for easy interpretation. SOM has two layers, one is the Input
# layer and the other one is the Output layer.
# Let’s say an input data of size (m, n) where m is the number of training examples and n is
# the number of features in each example. First, it initializes the weights of size (n, C) where
# C is the number of clusters. Then iterating over the input data, for each training example, it
# updates the winning vector (weight vector with the shortest distance (e.g Euclidean
# distance) from training example). Weight updation rule is given by :
# wij = wij(old) + alpha(t) * (xi
# k – wij(old))
# where alpha is a learning rate at time t, j denotes the winning vector, i denotes the ith feature
# of training example and k denotes the k
# th training example from the input data. After
# training the SOM network, trained weights are used for clustering new examples. A new
# example falls in the cluster of winning vectors.
